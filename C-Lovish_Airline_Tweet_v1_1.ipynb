{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 742210,
          "sourceType": "datasetVersion",
          "datasetId": 17
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# General Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP Libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Transformers and Tokenization\n",
        "!pip install -q transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Explainable AI Libraries\n",
        "!pip install -q lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data with checks\n",
        "import os\n",
        "\n",
        "nltk_data_path = os.path.join(os.path.expanduser('~'), 'nltk_data')\n",
        "\n",
        "if not os.path.exists(nltk_data_path):\n",
        "    os.makedirs(nltk_data_path)\n",
        "\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "required_packages = ['stopwords', 'wordnet', 'omw-1.4']\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        nltk.data.find(f'corpora/{package}')\n",
        "    except LookupError:\n",
        "        print(f'Downloading NLTK package: {package}')\n",
        "        nltk.download(package, download_dir=nltk_data_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:15.449877Z",
          "iopub.execute_input": "2024-09-23T20:55:15.450595Z",
          "iopub.status.idle": "2024-09-23T20:55:42.359909Z",
          "shell.execute_reply.started": "2024-09-23T20:55:15.450552Z",
          "shell.execute_reply": "2024-09-23T20:55:42.358625Z"
        },
        "trusted": true,
        "id": "c5gvjNGLjYy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data packages\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Necessary for WordNet lemmatizer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:42.362357Z",
          "iopub.execute_input": "2024-09-23T20:55:42.362688Z",
          "iopub.status.idle": "2024-09-23T20:55:42.450869Z",
          "shell.execute_reply.started": "2024-09-23T20:55:42.362652Z",
          "shell.execute_reply": "2024-09-23T20:55:42.449410Z"
        },
        "trusted": true,
        "id": "TEIzL5U5jYy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:42.544504Z",
          "iopub.execute_input": "2024-09-23T20:55:42.544961Z",
          "iopub.status.idle": "2024-09-23T20:55:42.665517Z",
          "shell.execute_reply.started": "2024-09-23T20:55:42.544909Z",
          "shell.execute_reply": "2024-09-23T20:55:42.664437Z"
        },
        "trusted": true,
        "id": "zWL6z_M9jYzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the dataset\n",
        "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Distribution of sentiments\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='airline_sentiment', data=df, palette='viridis')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of sentiments per airline\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='airline', hue='airline_sentiment', data=df, palette='viridis')\n",
        "plt.title('Sentiment Distribution per Airline')\n",
        "plt.xlabel('Airline')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()\n",
        "\n",
        "# Pie chart for sentiment distribution\n",
        "plt.figure(figsize=(6,6))\n",
        "df['airline_sentiment'].value_counts().plot.pie(autopct='%1.1f%%', colors=sns.color_palette('viridis', 3), startangle=90, explode=[0.05]*3)\n",
        "plt.title('Sentiment Distribution Pie Chart')\n",
        "plt.ylabel('')  # To remove the y-label\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:42.668472Z",
          "iopub.execute_input": "2024-09-23T20:55:42.669407Z",
          "iopub.status.idle": "2024-09-23T20:55:43.507336Z",
          "shell.execute_reply.started": "2024-09-23T20:55:42.669361Z",
          "shell.execute_reply": "2024-09-23T20:55:43.503975Z"
        },
        "trusted": true,
        "id": "jJia4qg5jYzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Check if text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove user @ references and '#' from hashtags\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "\n",
        "    # Remove special characters, numbers, and punctuations\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\d', ' ', text)\n",
        "\n",
        "    # Remove single characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "\n",
        "    # Remove multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Apply preprocessing\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Display cleaned text\n",
        "df[['text', 'clean_text']].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:43.509009Z",
          "iopub.execute_input": "2024-09-23T20:55:43.509583Z",
          "iopub.status.idle": "2024-09-23T20:55:44.130875Z",
          "shell.execute_reply.started": "2024-09-23T20:55:43.509517Z",
          "shell.execute_reply": "2024-09-23T20:55:44.129598Z"
        },
        "trusted": true,
        "id": "1FqBckB2jYzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define features and labels\n",
        "X = df['clean_text']\n",
        "y = df['airline_sentiment'].map({'negative':0, 'neutral':1, 'positive':2})  # Encoding sentiments\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:44.132295Z",
          "iopub.execute_input": "2024-09-23T20:55:44.132619Z",
          "iopub.status.idle": "2024-09-23T20:55:44.996993Z",
          "shell.execute_reply.started": "2024-09-23T20:55:44.132587Z",
          "shell.execute_reply": "2024-09-23T20:55:44.995782Z"
        },
        "trusted": true,
        "id": "nbTZTqCHjYzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentiments\n",
        "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['label'] = df['airline_sentiment'].map(label_mapping)\n",
        "\n",
        "# Display label distribution with custom colors\n",
        "colors = ['#FF5733', '#85df6d', '#007BFF']  # Example colors: red, light green, and blue\n",
        "\n",
        "df['label'].value_counts().sort_index().plot(kind='bar', color=colors)\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'], rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:44.998570Z",
          "iopub.execute_input": "2024-09-23T20:55:44.998967Z",
          "iopub.status.idle": "2024-09-23T20:55:45.240329Z",
          "shell.execute_reply.started": "2024-09-23T20:55:44.998921Z",
          "shell.execute_reply": "2024-09-23T20:55:45.239273Z"
        },
        "trusted": true,
        "id": "zHsOXFx7jYza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and labels\n",
        "X = df['clean_text']\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:45.242120Z",
          "iopub.execute_input": "2024-09-23T20:55:45.242546Z",
          "iopub.status.idle": "2024-09-23T20:55:45.262798Z",
          "shell.execute_reply.started": "2024-09-23T20:55:45.242498Z",
          "shell.execute_reply": "2024-09-23T20:55:45.261819Z"
        },
        "trusted": true,
        "id": "zGzblNgfjYzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(\n",
        "        texts.tolist(),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Tokenize training and testing data\n",
        "train_encodings = tokenize_function(X_train)\n",
        "test_encodings = tokenize_function(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:45.267235Z",
          "iopub.execute_input": "2024-09-23T20:55:45.267678Z",
          "iopub.status.idle": "2024-09-23T20:55:54.698065Z",
          "shell.execute_reply.started": "2024-09-23T20:55:45.267625Z",
          "shell.execute_reply": "2024-09-23T20:55:54.696783Z"
        },
        "trusted": true,
        "id": "N9hD87vgjYzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TweetsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = TweetsDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = TweetsDataset(test_encodings, y_test.tolist())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:54.699544Z",
          "iopub.execute_input": "2024-09-23T20:55:54.699970Z",
          "iopub.status.idle": "2024-09-23T20:55:54.709712Z",
          "shell.execute_reply.started": "2024-09-23T20:55:54.699921Z",
          "shell.execute_reply": "2024-09-23T20:55:54.708778Z"
        },
        "trusted": true,
        "id": "VPQ26yxKjYzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Initialize the model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=3,  # Negative, Neutral, Positive\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,              # Number of training epochs\n",
        "    per_device_train_batch_size=64,  # Batch size for training\n",
        "    per_device_eval_batch_size=64,   # Batch size for evaluation\n",
        "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # Strength of weight decay\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Define a metric computation function\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:54.711000Z",
          "iopub.execute_input": "2024-09-23T20:55:54.711374Z",
          "iopub.status.idle": "2024-09-23T20:55:55.284072Z",
          "shell.execute_reply.started": "2024-09-23T20:55:54.711340Z",
          "shell.execute_reply": "2024-09-23T20:55:55.282940Z"
        },
        "trusted": true,
        "id": "wT4BYvY7jYzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:55:55.285498Z",
          "iopub.execute_input": "2024-09-23T20:55:55.285844Z",
          "iopub.status.idle": "2024-09-23T20:59:45.773064Z",
          "shell.execute_reply.started": "2024-09-23T20:55:55.285796Z",
          "shell.execute_reply": "2024-09-23T20:59:45.772202Z"
        },
        "trusted": true,
        "id": "wTSO6BRejYz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:59:45.774187Z",
          "iopub.execute_input": "2024-09-23T20:59:45.774463Z",
          "iopub.status.idle": "2024-09-23T20:59:49.386436Z",
          "shell.execute_reply.started": "2024-09-23T20:59:45.774433Z",
          "shell.execute_reply": "2024-09-23T20:59:49.385338Z"
        },
        "trusted": true,
        "id": "AaFAPkkyjYz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Predictions\n",
        "predictions, true_labels, _ = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, preds, target_names=['Negative', 'Neutral', 'Positive']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Create a custom colormap\n",
        "cmap = mcolors.ListedColormap(['#ffffff', '#85df6d', '#3cb371', '#006400'])  # White to light green to dark green\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
        "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:59:49.387723Z",
          "iopub.execute_input": "2024-09-23T20:59:49.388081Z",
          "iopub.status.idle": "2024-09-23T20:59:53.423140Z",
          "shell.execute_reply.started": "2024-09-23T20:59:49.388046Z",
          "shell.execute_reply": "2024-09-23T20:59:53.421997Z"
        },
        "trusted": true,
        "id": "Moek4FrFjYz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explainable AI"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:59:53.425259Z",
          "iopub.execute_input": "2024-09-23T20:59:53.426178Z",
          "iopub.status.idle": "2024-09-23T20:59:53.433010Z",
          "shell.execute_reply.started": "2024-09-23T20:59:53.426113Z",
          "shell.execute_reply": "2024-09-23T20:59:53.430831Z"
        },
        "trusted": true,
        "id": "fef-0kBLjYz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Define class names\n",
        "class_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Initialize LIME explainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:59:53.434599Z",
          "iopub.execute_input": "2024-09-23T20:59:53.435202Z",
          "iopub.status.idle": "2024-09-23T20:59:53.442892Z",
          "shell.execute_reply.started": "2024-09-23T20:59:53.435153Z",
          "shell.execute_reply": "2024-09-23T20:59:53.441832Z"
        },
        "trusted": true,
        "id": "qbf7-LwhjY0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(texts):\n",
        "    # Tokenize the texts\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    # Move tensors to the same device as the model\n",
        "    encodings = {key: val.to(model.device) for key, val in encodings.items()}\n",
        "\n",
        "    # Get logits from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    return probs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:59:53.444100Z",
          "iopub.execute_input": "2024-09-23T20:59:53.444532Z",
          "iopub.status.idle": "2024-09-23T20:59:53.455639Z",
          "shell.execute_reply.started": "2024-09-23T20:59:53.444483Z",
          "shell.execute_reply": "2024-09-23T20:59:53.454124Z"
        },
        "trusted": true,
        "id": "m0gkNVd1jY0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a sample from the test set\n",
        "sample_idx = 42\n",
        "sample_text = X_test.iloc[sample_idx]\n",
        "actual_sentiment = y_test.iloc[sample_idx]\n",
        "predicted_sentiment = preds[sample_idx]\n",
        "\n",
        "print(f\"Original Tweet: {df.iloc[X_test.index[sample_idx]]['text']}\")\n",
        "print(f\"Cleaned Tweet: {sample_text}\")\n",
        "print(f\"Actual Sentiment: {class_names[actual_sentiment]}\")\n",
        "print(f\"Predicted Sentiment: {class_names[predicted_sentiment]}\")\n",
        "\n",
        "# Generate explanation\n",
        "exp = explainer.explain_instance(\n",
        "    sample_text,\n",
        "    predict_proba,\n",
        "    num_features=10,\n",
        "    labels=[predicted_sentiment]\n",
        ")\n",
        "\n",
        "# Display the explanation\n",
        "exp.show_in_notebook(text=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:03:09.109644Z",
          "iopub.execute_input": "2024-09-23T21:03:09.110066Z",
          "iopub.status.idle": "2024-09-23T21:03:14.827071Z",
          "shell.execute_reply.started": "2024-09-23T21:03:09.110023Z",
          "shell.execute_reply": "2024-09-23T21:03:14.826101Z"
        },
        "trusted": true,
        "id": "jrv_8RE1jY0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display explanations for multiple samples\n",
        "def display_lime_explanations(texts, labels, preds, num_samples=3):\n",
        "    for i in range(num_samples):\n",
        "        idx = np.random.randint(0, len(texts))\n",
        "        text = texts.iloc[idx]\n",
        "        actual = labels.iloc[idx]\n",
        "        pred = preds[idx]\n",
        "        print(f\"\\nSample {i+1}\")\n",
        "        print(f\"Original Tweet: {df.iloc[X_test.index[idx]]['text']}\")\n",
        "        print(f\"Cleaned Tweet: {text}\")\n",
        "        print(f\"Actual Sentiment: {class_names[actual]}\")\n",
        "        print(f\"Predicted Sentiment: {class_names[pred]}\")\n",
        "        exp = explainer.explain_instance(\n",
        "            text,\n",
        "            predict_proba,\n",
        "            num_features=10,\n",
        "            labels=[pred]\n",
        "        )\n",
        "        exp.show_in_notebook(text=True)\n",
        "\n",
        "# Display explanations for 3 random samples\n",
        "display_lime_explanations(X_test, y_test, preds, num_samples=3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:01:53.135636Z",
          "iopub.execute_input": "2024-09-23T21:01:53.136568Z",
          "iopub.status.idle": "2024-09-23T21:02:02.702667Z",
          "shell.execute_reply.started": "2024-09-23T21:01:53.136528Z",
          "shell.execute_reply": "2024-09-23T21:02:02.696180Z"
        },
        "trusted": true,
        "id": "08CYNBL8jY0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}